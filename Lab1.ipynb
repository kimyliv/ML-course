{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO4dzbsJsLUQjKKZeSVCDhw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","sys.path.append('/content/drive/MyDrive/Colab_Notebooks')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ku197D8seXc7","executionInfo":{"status":"ok","timestamp":1744526773464,"user_tz":-120,"elapsed":24330,"user":{"displayName":"k-j yli","userId":"05176293678065859194"}},"outputId":"d9ad6636-4e6b-433f-f121-b68edaf8f634"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# **Task 1.1 - TF-IDF + ANN**"],"metadata":{"id":"feG6l2sK9jea"}},{"cell_type":"markdown","source":["# Small dataset"],"metadata":{"id":"eHeop597OHtA"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744526874830,"user_tz":-120,"elapsed":17640,"user":{"displayName":"k-j yli","userId":"05176293678065859194"}},"outputId":"32540021-8b08-4ad4-8289-79dcdddbca93","id":"oi8L9geiv86N"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import nltk\n","\n","# Download necessary NLTK resources\n","nltk.download('punkt_tab')\n","nltk.download('stopwords')\n","nltk.download('punkt_tab')\n","from nltk.corpus import stopwords\n","from nltk import word_tokenize\n","from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report\n","\n","def preprocess_pandas(data, columns):\n","    # Create a new dataframe\n","    df_ = pd.DataFrame(columns=columns)\n","    data['Sentence'] = data['Sentence'].str.lower()\n","    data['Sentence'] = data['Sentence'].replace('[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', regex=True)                      # remove emails\n","    data['Sentence'] = data['Sentence'].replace('((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', regex=True)    # remove IP addresses\n","    data['Sentence'] = data['Sentence'].str.replace('[^\\w\\s]', '', regex=True)                                           # remove special characters\n","    data['Sentence'] = data['Sentence'].replace('\\d', '', regex=True)                                                    # remove numbers\n","    for index, row in data.iterrows():\n","        word_tokens = word_tokenize(row['Sentence'])\n","        filtered_sent = [w for w in word_tokens if w not in stopwords.words('english')]\n","        df_.loc[len(df_)] = {\n","            \"index\": row['index'],\n","            \"Class\": row['Class'],\n","            \"Sentence\": \" \".join(filtered_sent)\n","        }\n","    return df_  # It makes sense to return the new preprocessed DataFrame\n","\n","if __name__ == \"__main__\":\n","    # Load data and assign column names\n","    data = pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/amazon_cells_labelled.txt\",\n","                       delimiter='\\t', header=None)\n","    data.columns = ['Sentence', 'Class']\n","    data['index'] = data.index  # add new column index\n","    columns = ['index', 'Class', 'Sentence']\n","\n","    # Pre-process the data\n","    data_preprocessed = preprocess_pandas(data, columns)\n","\n","    # First, split the full dataset into training (80%) and temporary (20%) sets.\n","    train_sentences, temp_sentences, train_labels, temp_labels = train_test_split(\n","        data_preprocessed['Sentence'].values.astype('U'),\n","        data_preprocessed['Class'].values.astype('int32'),\n","        test_size=0.2,          # 20% goes into the temporary set\n","        random_state=0,\n","        shuffle=True\n","    )\n","\n","    # Then split the temporary set equally into validation and test sets (10% each overall).\n","    # You can adjust the ratio depending on your requirements.\n","    validation_sentences, test_sentences, validation_labels, test_labels = train_test_split(\n","        temp_sentences,\n","        temp_labels,\n","        test_size=0.5,          # 50% of the temporary set becomes test set, the rest validation\n","        random_state=0,\n","        shuffle=True\n","    )\n","\n","    # Vectorize the text using TFIDF; fit only on the training data.\n","    word_vectorizer = TfidfVectorizer(\n","        analyzer='word',\n","        ngram_range=(1, 2),\n","        max_features=30000,\n","        max_df=0.5,\n","        use_idf=True,\n","        norm='l2'\n","    )\n","    # Fit and transform the training sentences\n","    train_data = word_vectorizer.fit_transform(train_sentences)\n","    train_data = train_data.todense()  # Convert sparse matrix to dense\n","\n","    # Transform validation and test sets using the fitted vectorizer\n","    validation_data = word_vectorizer.transform(validation_sentences)\n","    validation_data = validation_data.todense()\n","\n","    test_data = word_vectorizer.transform(test_sentences)\n","    test_data = test_data.todense()\n","\n","    # Convert the data to PyTorch tensors\n","    train_x_tensor = torch.from_numpy(np.array(train_data)).type(torch.FloatTensor)\n","    train_y_tensor = torch.from_numpy(np.array(train_labels)).long()\n","\n","    validation_x_tensor = torch.from_numpy(np.array(validation_data)).type(torch.FloatTensor)\n","    validation_y_tensor = torch.from_numpy(np.array(validation_labels)).long()\n","\n","    test_x_tensor = torch.from_numpy(np.array(test_data)).type(torch.FloatTensor)\n","    test_y_tensor = torch.from_numpy(np.array(test_labels)).long()\n","\n","    # Optionally, you can create DataLoader objects for batching during training:\n","    batch_size = 32\n","\n","    train_dataset = TensorDataset(train_x_tensor, train_y_tensor)\n","    validation_dataset = TensorDataset(validation_x_tensor, validation_y_tensor)\n","    test_dataset = TensorDataset(test_x_tensor, test_y_tensor)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","# Set the device to GPU if available, otherwise CPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Define a simple ANN model for binary classification\n","class SimpleANN(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(SimpleANN, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        out = self.fc1(x)\n","        out = self.relu(out)\n","        out = self.fc2(out)\n","        return out\n","\n","# Set model parameters\n","input_dim = train_x_tensor.shape[1]  # number of TF-IDF features\n","hidden_dim = 20                     # experiment with this value as needed\n","output_dim = 2                       # 0 (negative) or 1 (positive)\n","\n","# Instantiate the model, loss function, and optimizer, and move the model to the device\n","model = SimpleANN(input_dim, hidden_dim, output_dim).to(device)\n","criterion = nn.CrossEntropyLoss()  # suitable for multi-class classification\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Early Stopping Parameters\n","early_stopping_patience = 3  # number of epochs with no improvement after which training will be stopped\n","best_val_loss = float('inf')\n","epochs_no_improve = 0\n","\n","# Train the model\n","num_epochs = 50  # you can adjust the number of epochs\n","for epoch in range(num_epochs):\n","    model.train()\n","    epoch_train_loss = 0.0\n","    for batch_x, batch_y in train_loader:\n","        # Move data to the GPU\n","        batch_x = batch_x.to(device)\n","        batch_y = batch_y.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(batch_x)\n","        loss = criterion(outputs, batch_y)\n","        loss.backward()\n","        optimizer.step()\n","        epoch_train_loss += loss.item()\n","\n","    avg_train_loss = epoch_train_loss / len(train_loader)\n","\n","    # Validation phase (Monitoring Performance)\n","    model.eval()\n","    epoch_val_loss = 0.0\n","    correct_val = 0\n","    total_val = 0\n","    with torch.no_grad():\n","        for val_x, val_y in validation_loader:\n","            val_x = val_x.to(device)\n","            val_y = val_y.to(device)\n","\n","            outputs = model(val_x)\n","            loss = criterion(outputs, val_y)\n","            epoch_val_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total_val += val_y.size(0)\n","            correct_val += (predicted == val_y).sum().item()\n","\n","    avg_val_loss = epoch_val_loss / len(validation_loader)\n","    val_accuracy = 100 * correct_val / total_val\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs}] -> '\n","          f'Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | '\n","          f'Val Accuracy: {val_accuracy:.2f}%')\n","\n","    # Early Stopping and Checkpointing:\n","    if avg_val_loss < best_val_loss:\n","        best_val_loss = avg_val_loss\n","        epochs_no_improve = 0\n","        # Save model checkpoint when validation loss improves\n","        torch.save(model.state_dict(), 'best_model.pt')\n","        print(\"Model improved. Checkpoint saved.\")\n","    else:\n","        epochs_no_improve += 1\n","        if epochs_no_improve >= early_stopping_patience:\n","            print(\"Early stopping triggered: Validation loss has not improved for \"\n","                  f\"{early_stopping_patience} consecutive epochs.\")\n","            break  # Stop training if no improvement\n","\n","# Load the best model checkpoint for testing\n","model.load_state_dict(torch.load('best_model.pt'))\n","model.eval()\n","correct_test = 0\n","total_test = 0\n","with torch.no_grad():\n","    for test_x, test_y in test_loader:\n","        test_x = test_x.to(device)\n","        test_y = test_y.to(device)\n","\n","        outputs = model(test_x)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total_test += test_y.size(0)\n","        correct_test += (predicted == test_y).sum().item()\n","\n","test_accuracy = 100 * correct_test / total_test\n","print(f'Test Accuracy: {test_accuracy:.2f}%')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744526930749,"user_tz":-120,"elapsed":4691,"user":{"displayName":"k-j yli","userId":"05176293678065859194"}},"outputId":"46bea77d-0a59-438b-8f25-69fd32a5487e","id":"OKTGnmE3wBs5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","Epoch [1/50] -> Train Loss: 0.6936 | Val Loss: 0.6972 | Val Accuracy: 46.00%\n","Model improved. Checkpoint saved.\n","Epoch [2/50] -> Train Loss: 0.6808 | Val Loss: 0.6828 | Val Accuracy: 59.00%\n","Model improved. Checkpoint saved.\n","Epoch [3/50] -> Train Loss: 0.6494 | Val Loss: 0.6556 | Val Accuracy: 83.00%\n","Model improved. Checkpoint saved.\n","Epoch [4/50] -> Train Loss: 0.5916 | Val Loss: 0.6190 | Val Accuracy: 77.00%\n","Model improved. Checkpoint saved.\n","Epoch [5/50] -> Train Loss: 0.5125 | Val Loss: 0.5739 | Val Accuracy: 78.00%\n","Model improved. Checkpoint saved.\n","Epoch [6/50] -> Train Loss: 0.4229 | Val Loss: 0.5324 | Val Accuracy: 80.00%\n","Model improved. Checkpoint saved.\n","Epoch [7/50] -> Train Loss: 0.3370 | Val Loss: 0.4935 | Val Accuracy: 79.00%\n","Model improved. Checkpoint saved.\n","Epoch [8/50] -> Train Loss: 0.2636 | Val Loss: 0.4612 | Val Accuracy: 82.00%\n","Model improved. Checkpoint saved.\n","Epoch [9/50] -> Train Loss: 0.2059 | Val Loss: 0.4385 | Val Accuracy: 81.00%\n","Model improved. Checkpoint saved.\n","Epoch [10/50] -> Train Loss: 0.1619 | Val Loss: 0.4188 | Val Accuracy: 81.00%\n","Model improved. Checkpoint saved.\n","Epoch [11/50] -> Train Loss: 0.1291 | Val Loss: 0.4066 | Val Accuracy: 82.00%\n","Model improved. Checkpoint saved.\n","Epoch [12/50] -> Train Loss: 0.1048 | Val Loss: 0.3946 | Val Accuracy: 82.00%\n","Model improved. Checkpoint saved.\n","Epoch [13/50] -> Train Loss: 0.0864 | Val Loss: 0.3885 | Val Accuracy: 82.00%\n","Model improved. Checkpoint saved.\n","Epoch [14/50] -> Train Loss: 0.0723 | Val Loss: 0.3818 | Val Accuracy: 82.00%\n","Model improved. Checkpoint saved.\n","Epoch [15/50] -> Train Loss: 0.0614 | Val Loss: 0.3760 | Val Accuracy: 82.00%\n","Model improved. Checkpoint saved.\n","Epoch [16/50] -> Train Loss: 0.0527 | Val Loss: 0.3727 | Val Accuracy: 82.00%\n","Model improved. Checkpoint saved.\n","Epoch [17/50] -> Train Loss: 0.0457 | Val Loss: 0.3689 | Val Accuracy: 82.00%\n","Model improved. Checkpoint saved.\n","Epoch [18/50] -> Train Loss: 0.0401 | Val Loss: 0.3654 | Val Accuracy: 82.00%\n","Model improved. Checkpoint saved.\n","Epoch [19/50] -> Train Loss: 0.0355 | Val Loss: 0.3655 | Val Accuracy: 81.00%\n","Epoch [20/50] -> Train Loss: 0.0316 | Val Loss: 0.3628 | Val Accuracy: 81.00%\n","Model improved. Checkpoint saved.\n","Epoch [21/50] -> Train Loss: 0.0285 | Val Loss: 0.3617 | Val Accuracy: 81.00%\n","Model improved. Checkpoint saved.\n","Epoch [22/50] -> Train Loss: 0.0256 | Val Loss: 0.3608 | Val Accuracy: 81.00%\n","Model improved. Checkpoint saved.\n","Epoch [23/50] -> Train Loss: 0.0232 | Val Loss: 0.3595 | Val Accuracy: 81.00%\n","Model improved. Checkpoint saved.\n","Epoch [24/50] -> Train Loss: 0.0212 | Val Loss: 0.3576 | Val Accuracy: 82.00%\n","Model improved. Checkpoint saved.\n","Epoch [25/50] -> Train Loss: 0.0195 | Val Loss: 0.3576 | Val Accuracy: 82.00%\n","Epoch [26/50] -> Train Loss: 0.0179 | Val Loss: 0.3574 | Val Accuracy: 82.00%\n","Model improved. Checkpoint saved.\n","Epoch [27/50] -> Train Loss: 0.0166 | Val Loss: 0.3570 | Val Accuracy: 82.00%\n","Model improved. Checkpoint saved.\n","Epoch [28/50] -> Train Loss: 0.0154 | Val Loss: 0.3567 | Val Accuracy: 82.00%\n","Model improved. Checkpoint saved.\n","Epoch [29/50] -> Train Loss: 0.0143 | Val Loss: 0.3569 | Val Accuracy: 82.00%\n","Epoch [30/50] -> Train Loss: 0.0134 | Val Loss: 0.3567 | Val Accuracy: 82.00%\n","Model improved. Checkpoint saved.\n","Epoch [31/50] -> Train Loss: 0.0125 | Val Loss: 0.3568 | Val Accuracy: 82.00%\n","Epoch [32/50] -> Train Loss: 0.0117 | Val Loss: 0.3563 | Val Accuracy: 83.00%\n","Model improved. Checkpoint saved.\n","Epoch [33/50] -> Train Loss: 0.0111 | Val Loss: 0.3567 | Val Accuracy: 83.00%\n","Epoch [34/50] -> Train Loss: 0.0105 | Val Loss: 0.3562 | Val Accuracy: 83.00%\n","Model improved. Checkpoint saved.\n","Epoch [35/50] -> Train Loss: 0.0099 | Val Loss: 0.3571 | Val Accuracy: 83.00%\n","Epoch [36/50] -> Train Loss: 0.0094 | Val Loss: 0.3568 | Val Accuracy: 83.00%\n","Epoch [37/50] -> Train Loss: 0.0089 | Val Loss: 0.3572 | Val Accuracy: 83.00%\n","Early stopping triggered: Validation loss has not improved for 3 consecutive epochs.\n","Test Accuracy: 82.00%\n"]}]},{"cell_type":"code","source":["# Explicit test: examine a single review and its prediction\n","\n","# Define a sample review (or use one from your dataset)\n","sample_review = \"This product exceeded my expectations and works great!\"\n","\n","# Preprocess the sample similar to your training data:\n","sample_review_lower = sample_review.lower()\n","\n","# Vectorize the sample using the same TF-IDF vectorizer (assumes that word_vectorizer is already fitted)\n","sample_vector = word_vectorizer.transform([sample_review_lower]).todense()\n","\n","# Convert the vector to a PyTorch tensor and move it to the same device as the model (GPU if available)\n","sample_tensor = torch.from_numpy(np.array(sample_vector)).type(torch.FloatTensor).to(device)\n","\n","# Set the model to evaluation mode and compute the prediction\n","model.eval()\n","with torch.no_grad():\n","    output = model(sample_tensor)\n","    _, predicted_label = torch.max(output, 1)\n","\n","# Print the input and its predicted class\n","print(\"Input review:\", sample_review)\n","print(\"Predicted class:\", predicted_label.item())\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744526938255,"user_tz":-120,"elapsed":20,"user":{"displayName":"k-j yli","userId":"05176293678065859194"}},"outputId":"4ffed49c-65cb-48a4-cdcb-970c0f2254ce","id":"kJEIcgPmwNpl"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input review: This product exceeded my expectations and works great!\n","Predicted class: 1\n"]}]},{"cell_type":"code","source":["# Explicit test: examine a single review and its prediction\n","\n","# Define a sample review (or use one from your dataset)\n","sample_review = \"I completely detest this product, despite its cheap price.\"\n","\n","# Preprocess the sample similar to your training data:\n","sample_review_lower = sample_review.lower()\n","\n","# Vectorize the sample using the same TF-IDF vectorizer (assumes that word_vectorizer is already fitted)\n","sample_vector = word_vectorizer.transform([sample_review_lower]).todense()\n","\n","# Convert the vector to a PyTorch tensor and move it to the same device as the model (GPU if available)\n","sample_tensor = torch.from_numpy(np.array(sample_vector)).type(torch.FloatTensor).to(device)\n","\n","# Set the model to evaluation mode and compute the prediction\n","model.eval()\n","with torch.no_grad():\n","    output = model(sample_tensor)\n","    _, predicted_label = torch.max(output, 1)\n","\n","# Print the input and its predicted class\n","print(\"Input review:\", sample_review)\n","print(\"Predicted class:\", predicted_label.item())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744526941331,"user_tz":-120,"elapsed":25,"user":{"displayName":"k-j yli","userId":"05176293678065859194"}},"outputId":"71c31b52-9878-46c5-9610-728d9b949666","id":"lKlHpD_AwQv3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input review: I completely detest this product, despite its cheap price.\n","Predicted class: 0\n"]}]},{"cell_type":"markdown","source":["# Large dataset"],"metadata":{"id":"QCPNQqhhOa-j"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XHmrjofUd8ko","executionInfo":{"status":"ok","timestamp":1744527142097,"user_tz":-120,"elapsed":117242,"user":{"displayName":"k-j yli","userId":"05176293678065859194"}},"outputId":"74bd2c69-5d14-4fb0-9b06-13d19c130a8f"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import nltk\n","\n","# Download necessary NLTK resources\n","nltk.download('punkt_tab')\n","nltk.download('stopwords')\n","nltk.download('punkt_tab')\n","from nltk.corpus import stopwords\n","from nltk import word_tokenize\n","from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report\n","\n","def preprocess_pandas(data, columns):\n","    # Create a new dataframe\n","    df_ = pd.DataFrame(columns=columns)\n","    data['Sentence'] = data['Sentence'].str.lower()\n","    data['Sentence'] = data['Sentence'].replace('[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', regex=True)                      # remove emails\n","    data['Sentence'] = data['Sentence'].replace('((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', regex=True)    # remove IP addresses\n","    data['Sentence'] = data['Sentence'].str.replace('[^\\w\\s]', '', regex=True)                                           # remove special characters\n","    data['Sentence'] = data['Sentence'].replace('\\d', '', regex=True)                                                    # remove numbers\n","    for index, row in data.iterrows():\n","        word_tokens = word_tokenize(row['Sentence'])\n","        filtered_sent = [w for w in word_tokens if w not in stopwords.words('english')]\n","        df_.loc[len(df_)] = {\n","            \"index\": row['index'],\n","            \"Class\": row['Class'],\n","            \"Sentence\": \" \".join(filtered_sent)\n","        }\n","    return df_  # It makes sense to return the new preprocessed DataFrame\n","\n","if __name__ == \"__main__\":\n","    # Load data and assign column names\n","    data = pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/amazon_cells_labelled_LARGE_25K.txt\",\n","                       delimiter='\\t', header=None)\n","    data.columns = ['Sentence', 'Class']\n","    data['index'] = data.index  # add new column index\n","    columns = ['index', 'Class', 'Sentence']\n","\n","    # Pre-process the data\n","    data_preprocessed = preprocess_pandas(data, columns)\n","\n","    # First, split the full dataset into training (60%) and temporary (40%) sets.\n","    train_sentences, temp_sentences, train_labels, temp_labels = train_test_split(\n","        data_preprocessed['Sentence'].values.astype('U'),\n","        data_preprocessed['Class'].values.astype('int32'),\n","        test_size=0.4,          # 40% goes into the temporary set\n","        random_state=0,\n","        shuffle=True\n","    )\n","\n","    # Then split the temporary set equally into validation and test sets (20% each overall).\n","    # You can adjust the ratio depending on your requirements.\n","    validation_sentences, test_sentences, validation_labels, test_labels = train_test_split(\n","        temp_sentences,\n","        temp_labels,\n","        test_size=0.5,          # 50% of the temporary set becomes test set, the rest validation\n","        random_state=0,\n","        shuffle=True\n","    )\n","\n","    # Vectorize the text using TFIDF; fit only on the training data.\n","    word_vectorizer = TfidfVectorizer(\n","        analyzer='word',\n","        ngram_range=(1, 2),\n","        max_features=30000,\n","        max_df=0.5,\n","        use_idf=True,\n","        norm='l2'\n","    )\n","    # Fit and transform the training sentences\n","    train_data = word_vectorizer.fit_transform(train_sentences)\n","    train_data = train_data.todense()  # Convert sparse matrix to dense\n","\n","    # Transform validation and test sets using the fitted vectorizer\n","    validation_data = word_vectorizer.transform(validation_sentences)\n","    validation_data = validation_data.todense()\n","\n","    test_data = word_vectorizer.transform(test_sentences)\n","    test_data = test_data.todense()\n","\n","    # Convert the data to PyTorch tensors\n","    train_x_tensor = torch.from_numpy(np.array(train_data)).type(torch.FloatTensor)\n","    train_y_tensor = torch.from_numpy(np.array(train_labels)).long()\n","\n","    validation_x_tensor = torch.from_numpy(np.array(validation_data)).type(torch.FloatTensor)\n","    validation_y_tensor = torch.from_numpy(np.array(validation_labels)).long()\n","\n","    test_x_tensor = torch.from_numpy(np.array(test_data)).type(torch.FloatTensor)\n","    test_y_tensor = torch.from_numpy(np.array(test_labels)).long()\n","\n","    # Optionally, you can create DataLoader objects for batching during training:\n","    batch_size = 32\n","\n","    train_dataset = TensorDataset(train_x_tensor, train_y_tensor)\n","    validation_dataset = TensorDataset(validation_x_tensor, validation_y_tensor)\n","    test_dataset = TensorDataset(test_x_tensor, test_y_tensor)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","class SimpleANN(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate=0.5):\n","        super(SimpleANN, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        self.bn1 = nn.BatchNorm1d(hidden_dim)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.fc2 = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        out = self.fc1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.dropout(out)\n","        out = self.fc2(out)\n","        return out\n","\n","# Assume train_x_tensor is already defined\n","input_dim = train_x_tensor.shape[1]\n","hidden_dim = 20\n","output_dim = 2\n","\n","model = SimpleANN(input_dim, hidden_dim, output_dim, dropout_rate=0.5).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n","\n","\n","# Early Stopping Parameters\n","early_stopping_patience = 3  # number of epochs with no improvement after which training will be stopped\n","best_val_loss = float('inf')\n","epochs_no_improve = 0\n","\n","# Train the model\n","num_epochs = 100  # you can adjust the number of epochs\n","for epoch in range(num_epochs):\n","    model.train()\n","    epoch_train_loss = 0.0\n","    for batch_x, batch_y in train_loader:\n","        # Move data to the GPU\n","        batch_x = batch_x.to(device)\n","        batch_y = batch_y.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(batch_x)\n","        loss = criterion(outputs, batch_y)\n","        loss.backward()\n","        optimizer.step()\n","        epoch_train_loss += loss.item()\n","\n","    avg_train_loss = epoch_train_loss / len(train_loader)\n","\n","    # Validation phase (Monitoring Performance)\n","    model.eval()\n","    epoch_val_loss = 0.0\n","    correct_val = 0\n","    total_val = 0\n","    with torch.no_grad():\n","        for val_x, val_y in validation_loader:\n","            val_x = val_x.to(device)\n","            val_y = val_y.to(device)\n","\n","            outputs = model(val_x)\n","            loss = criterion(outputs, val_y)\n","            epoch_val_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total_val += val_y.size(0)\n","            correct_val += (predicted == val_y).sum().item()\n","\n","    avg_val_loss = epoch_val_loss / len(validation_loader)\n","    val_accuracy = 100 * correct_val / total_val\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs}] -> '\n","          f'Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | '\n","          f'Val Accuracy: {val_accuracy:.2f}%')\n","\n","    # Early Stopping and Checkpointing:\n","    if avg_val_loss < best_val_loss:\n","        best_val_loss = avg_val_loss\n","        epochs_no_improve = 0\n","        # Save model checkpoint when validation loss improves\n","        torch.save(model.state_dict(), 'best_model.pt')\n","        print(\"Model improved. Checkpoint saved.\")\n","    else:\n","        epochs_no_improve += 1\n","        if epochs_no_improve >= early_stopping_patience:\n","            print(\"Early stopping triggered: Validation loss has not improved for \"\n","                  f\"{early_stopping_patience} consecutive epochs.\")\n","            break  # Stop training if no improvement\n","\n","# Load the best model checkpoint for testing\n","model.load_state_dict(torch.load('best_model.pt'))\n","model.eval()\n","correct_test = 0\n","total_test = 0\n","with torch.no_grad():\n","    for test_x, test_y in test_loader:\n","        test_x = test_x.to(device)\n","        test_y = test_y.to(device)\n","\n","        outputs = model(test_x)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total_test += test_y.size(0)\n","        correct_test += (predicted == test_y).sum().item()\n","\n","test_accuracy = 100 * correct_test / total_test\n","print(f'Test Accuracy: {test_accuracy:.2f}%')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ss2vKdHcgEnR","executionInfo":{"status":"ok","timestamp":1744527203874,"user_tz":-120,"elapsed":32513,"user":{"displayName":"k-j yli","userId":"05176293678065859194"}},"outputId":"531a9108-04c6-4901-9714-ed7b267fefac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/100] -> Train Loss: 0.4540 | Val Loss: 0.3349 | Val Accuracy: 84.94%\n","Model improved. Checkpoint saved.\n","Epoch [2/100] -> Train Loss: 0.2775 | Val Loss: 0.3229 | Val Accuracy: 85.76%\n","Model improved. Checkpoint saved.\n","Epoch [3/100] -> Train Loss: 0.1842 | Val Loss: 0.3535 | Val Accuracy: 84.98%\n","Epoch [4/100] -> Train Loss: 0.1395 | Val Loss: 0.3845 | Val Accuracy: 84.88%\n","Epoch [5/100] -> Train Loss: 0.1119 | Val Loss: 0.4207 | Val Accuracy: 85.14%\n","Early stopping triggered: Validation loss has not improved for 3 consecutive epochs.\n","Test Accuracy: 85.44%\n"]}]},{"cell_type":"code","source":["# Explicit test: examine a single review and its prediction\n","\n","# Define a sample review (or use one from your dataset)\n","sample_review = \"This product exceeded my expectations and works great!\"\n","\n","# Preprocess the sample similar to your training data:\n","sample_review_lower = sample_review.lower()\n","\n","# Vectorize the sample using the same TF-IDF vectorizer (assumes that word_vectorizer is already fitted)\n","sample_vector = word_vectorizer.transform([sample_review_lower]).todense()\n","\n","# Convert the vector to a PyTorch tensor and move it to the same device as the model (GPU if available)\n","sample_tensor = torch.from_numpy(np.array(sample_vector)).type(torch.FloatTensor).to(device)\n","\n","# Set the model to evaluation mode and compute the prediction\n","model.eval()\n","with torch.no_grad():\n","    output = model(sample_tensor)\n","    _, predicted_label = torch.max(output, 1)\n","\n","# Print the input and its predicted class\n","print(\"Input review:\", sample_review)\n","print(\"Predicted class:\", predicted_label.item())\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dsaLzzVAbSIG","executionInfo":{"status":"ok","timestamp":1744527207939,"user_tz":-120,"elapsed":18,"user":{"displayName":"k-j yli","userId":"05176293678065859194"}},"outputId":"5aa9f445-9cd1-4333-81e6-b8c2e38ae726"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input review: This product exceeded my expectations and works great!\n","Predicted class: 1\n"]}]},{"cell_type":"code","source":["# Explicit test: examine a single review and its prediction\n","\n","# Define a sample review (or use one from your dataset)\n","sample_review = \"I completely detest this product, despite its cheap price.\"\n","\n","# Preprocess the sample similar to your training data:\n","sample_review_lower = sample_review.lower()\n","\n","# Vectorize the sample using the same TF-IDF vectorizer (assumes that word_vectorizer is already fitted)\n","sample_vector = word_vectorizer.transform([sample_review_lower]).todense()\n","\n","# Convert the vector to a PyTorch tensor and move it to the same device as the model (GPU if available)\n","sample_tensor = torch.from_numpy(np.array(sample_vector)).type(torch.FloatTensor).to(device)\n","\n","# Set the model to evaluation mode and compute the prediction\n","model.eval()\n","with torch.no_grad():\n","    output = model(sample_tensor)\n","    _, predicted_label = torch.max(output, 1)\n","\n","# Print the input and its predicted class\n","print(\"Input review:\", sample_review)\n","print(\"Predicted class:\", predicted_label.item())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744527209891,"user_tz":-120,"elapsed":45,"user":{"displayName":"k-j yli","userId":"05176293678065859194"}},"outputId":"635374ab-95e2-4162-ce60-073297d7deb2","id":"-oNbYKOqcAz7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input review: I completely detest this product, despite its cheap price.\n","Predicted class: 0\n"]}]},{"cell_type":"markdown","source":["# **Task 1.2 - Transformer implementation**"],"metadata":{"id":"HTndi8q4woRs"}},{"cell_type":"markdown","source":["# Small dataset"],"metadata":{"id":"PutHcFZGN8cp"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from transformers import AutoTokenizer\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('punkt_tab')\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords\n","\n","# Preprocessing function (adjust as needed)\n","def preprocess_pandas(data, columns):\n","    df_ = pd.DataFrame(columns=columns)\n","    data['Sentence'] = data['Sentence'].str.lower()\n","    data['Sentence'] = data['Sentence'].replace('[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', regex=True)\n","    data['Sentence'] = data['Sentence'].replace('((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', regex=True)\n","    data['Sentence'] = data['Sentence'].str.replace('[^\\w\\s]', '', regex=True)\n","    data['Sentence'] = data['Sentence'].replace('\\d', '', regex=True)\n","    for index, row in data.iterrows():\n","        word_tokens = word_tokenize(row['Sentence'])\n","        filtered_sent = [w for w in word_tokens if not w in stopwords.words('english')]\n","        df_.loc[len(df_)] = {\n","            \"index\": row['index'],\n","            \"Class\": row['Class'],\n","            \"Sentence\": \" \".join(filtered_sent)\n","        }\n","    return df_\n","\n","# Load and preprocess your data\n","data = pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/amazon_cells_labelled.txt\",\n","                   delimiter='\\t', header=None)\n","data.columns = ['Sentence', 'Class']\n","data['index'] = data.index\n","columns = ['index', 'Class', 'Sentence']\n","data = preprocess_pandas(data, columns)\n","\n","# First split: 80% training and 20% temporary data.\n","train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n","    data['Sentence'].values.astype('U'),\n","    data['Class'].values.astype('int32'),\n","    test_size=0.20,\n","    random_state=0,\n","    shuffle=True\n",")\n","\n","# Second split: split temporary data equally into 50% validation and 50% test (i.e. 10% each overall)\n","val_texts, test_texts, val_labels, test_labels = train_test_split(\n","    temp_texts,\n","    temp_labels,\n","    test_size=0.5,\n","    random_state=0,\n","    shuffle=True\n",")\n","\n","# Initialize the transformer tokenizer (e.g., BERT)\n","tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Tokenize texts for each set (with padding and truncation)\n","encoded_train = tokenizer(list(train_texts), padding=True, truncation=True, return_tensors='pt')\n","encoded_val = tokenizer(list(val_texts), padding=True, truncation=True, return_tensors='pt')\n","encoded_test = tokenizer(list(test_texts), padding=True, truncation=True, return_tensors='pt')\n","\n","# Convert labels to tensors\n","train_y_tensor = torch.tensor(train_labels).long()\n","val_y_tensor = torch.tensor(val_labels).long()\n","test_y_tensor = torch.tensor(test_labels).long()\n","\n","# Create TensorDatasets for training, validation, and test sets\n","train_dataset = TensorDataset(encoded_train['input_ids'],\n","                              encoded_train['attention_mask'],\n","                              train_y_tensor)\n","val_dataset = TensorDataset(encoded_val['input_ids'],\n","                            encoded_val['attention_mask'],\n","                            val_y_tensor)\n","test_dataset = TensorDataset(encoded_test['input_ids'],\n","                             encoded_test['attention_mask'],\n","                             test_y_tensor)\n","\n","# Create DataLoaders\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"],"metadata":{"executionInfo":{"status":"ok","timestamp":1744478219216,"user_tz":-120,"elapsed":2526,"user":{"displayName":"k-j yli","userId":"05176293678065859194"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3b8b500e-064b-4142-aa24-9a16a5a165d1","id":"u92NR5g_wpXe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from transformers import AutoTokenizer, AutoModel\n","\n","# Define a Transformer-based classifier model\n","class TransformerClassifier(nn.Module):\n","    def __init__(self, transformer_model_name, hidden_dim, output_dim):\n","        super(TransformerClassifier, self).__init__()\n","        # Load a pretrained transformer model (e.g., BERT)\n","        self.transformer = AutoModel.from_pretrained(transformer_model_name)\n","        # Classification head: a simple feedforward network\n","        self.fc1 = nn.Linear(self.transformer.config.hidden_size, hidden_dim)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, input_ids, attention_mask):\n","        # Get the transformer outputs\n","        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n","        # Use the [CLS] token embedding (first token) as the sentence representation\n","        cls_embedding = outputs.last_hidden_state[:, 0, :]\n","        # Pass through the classification head\n","        out = self.fc1(cls_embedding)\n","        out = self.relu(out)\n","        out = self.fc2(out)\n","        return out\n","\n","# Set model parameters\n","hidden_dim = 20          # You can experiment with this value\n","output_dim = 2           # Binary classification: 0 (negative), 1 (positive)\n","\n","# Instantiate the model\n","model = TransformerClassifier('bert-base-uncased', hidden_dim, output_dim)\n","\n","# Check if GPU is available and move the model to GPU if possible\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# Use a smaller learning rate for fine-tuning transformers\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=2e-5)\n","\n","# ---- Early Stopping Configuration ----\n","early_stopping_patience = 3  # Number of epochs with no improvement after which training will be stopped\n","best_val_loss = float('inf')\n","epochs_no_improve = 0\n","\n","# ---- Training Loop ----\n","num_epochs = 10  # Maximum number of epochs to train\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    train_loss = 0.0\n","    for batch in train_loader:\n","        # Unpack the batch and move tensors to GPU\n","        input_ids, attention_mask, labels = batch\n","        input_ids = input_ids.to(device)\n","        attention_mask = attention_mask.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()       # Reset gradients\n","        outputs = model(input_ids, attention_mask)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()\n","\n","    avg_train_loss = train_loss / len(train_loader)\n","\n","    # ---- Validation Phase ----\n","    model.eval()\n","    val_loss = 0.0\n","    correct_val = 0\n","    total_val = 0\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            input_ids, attention_mask, labels = batch\n","            input_ids = input_ids.to(device)\n","            attention_mask = attention_mask.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(input_ids, attention_mask)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","            _, predicted = torch.max(outputs, 1)\n","            total_val += labels.size(0)\n","            correct_val += (predicted == labels).sum().item()\n","\n","    avg_val_loss = val_loss / len(val_loader)\n","    val_accuracy = 100 * correct_val / total_val\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs}] -> Train Loss: {avg_train_loss:.4f} | '\n","          f'Val Loss: {avg_val_loss:.4f} | Val Accuracy: {val_accuracy:.2f}%')\n","\n","    # ---- Early Stopping Check ----\n","    if avg_val_loss < best_val_loss:\n","        best_val_loss = avg_val_loss\n","        epochs_no_improve = 0\n","        # Save the current best model checkpoint\n","        torch.save(model.state_dict(), 'best_model_transformer.pt')\n","        print(\"Validation loss improved, saving model checkpoint.\")\n","    else:\n","        epochs_no_improve += 1\n","        print(f\"No improvement in validation loss for {epochs_no_improve} consecutive epoch(s).\")\n","        if epochs_no_improve >= early_stopping_patience:\n","            print(\"Early stopping triggered. Stopping training.\")\n","            break\n","\n","# ---- Load Best Model & Test Evaluation ----\n","model.load_state_dict(torch.load('best_model_transformer.pt'))\n","model.eval()\n","correct_test = 0\n","total_test = 0\n","with torch.no_grad():\n","    for batch in test_loader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids = input_ids.to(device)\n","        attention_mask = attention_mask.to(device)\n","        labels = labels.to(device)\n","\n","        outputs = model(input_ids, attention_mask)\n","        _, predicted = torch.max(outputs, 1)\n","        total_test += labels.size(0)\n","        correct_test += (predicted == labels).sum().item()\n","\n","test_accuracy = 100 * correct_test / total_test\n","print(f'Test Accuracy: {test_accuracy:.2f}%')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744478339938,"user_tz":-120,"elapsed":50850,"user":{"displayName":"k-j yli","userId":"05176293678065859194"}},"outputId":"40109e5e-385f-45c1-ab4b-fd79eb07e48c","id":"tdYZwKdSws1N"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/10] -> Train Loss: 0.6602 | Val Loss: 0.5437 | Val Accuracy: 86.00%\n","Validation loss improved, saving model checkpoint.\n","Epoch [2/10] -> Train Loss: 0.4406 | Val Loss: 0.3435 | Val Accuracy: 83.00%\n","Validation loss improved, saving model checkpoint.\n","Epoch [3/10] -> Train Loss: 0.2677 | Val Loss: 0.2817 | Val Accuracy: 85.00%\n","Validation loss improved, saving model checkpoint.\n","Epoch [4/10] -> Train Loss: 0.1925 | Val Loss: 0.3518 | Val Accuracy: 82.00%\n","No improvement in validation loss for 1 consecutive epoch(s).\n","Epoch [5/10] -> Train Loss: 0.1560 | Val Loss: 0.3617 | Val Accuracy: 81.00%\n","No improvement in validation loss for 2 consecutive epoch(s).\n","Epoch [6/10] -> Train Loss: 0.0964 | Val Loss: 0.4897 | Val Accuracy: 84.00%\n","No improvement in validation loss for 3 consecutive epoch(s).\n","Early stopping triggered. Stopping training.\n","Test Accuracy: 87.00%\n"]}]},{"cell_type":"code","source":["# Explicit test: examine a single review and its prediction\n","\n","# Define a sample review (or use one from your dataset)\n","sample_sentence = \"This product exceeded my expectations and works great!\"\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","# Tokenize the sample sentence (using the same tokenizer as during training)\n","encoded_sample = tokenizer(sample_sentence, padding=True, truncation=True, return_tensors='pt')\n","\n","# Move the tokenized inputs to the device (GPU if available)\n","encoded_sample = {key: val.to(device) for key, val in encoded_sample.items()}\n","\n","# Pass the tokenized input through the model to get predictions\n","with torch.no_grad():\n","    outputs = model(encoded_sample['input_ids'], encoded_sample['attention_mask'])\n","    # Get the predicted class (0 for negative, 1 for positive)\n","    _, predicted = torch.max(outputs, 1)\n","\n","# Extract the predicted class value\n","predicted_class = predicted.item()\n","\n","print(\"Input sentence:\", sample_sentence)\n","print(\"Predicted class:\", predicted_class)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744478344970,"user_tz":-120,"elapsed":36,"user":{"displayName":"k-j yli","userId":"05176293678065859194"}},"outputId":"c4906c8e-93a1-44f8-91cc-d1edc48633cc","id":"J5F4mCXNxUYF"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input sentence: This product exceeded my expectations and works great!\n","Predicted class: 1\n"]}]},{"cell_type":"code","source":["# Define a sample review sentence\n","sample_sentence = \"I completely detest this product, despite its cheap price.\"\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","# Tokenize the sample sentence (using the same tokenizer as during training)\n","encoded_sample = tokenizer(sample_sentence, padding=True, truncation=True, return_tensors='pt')\n","\n","# Move the tokenized inputs to the device (GPU if available)\n","encoded_sample = {key: val.to(device) for key, val in encoded_sample.items()}\n","\n","# Pass the tokenized input through the model to get predictions\n","with torch.no_grad():\n","    outputs = model(encoded_sample['input_ids'], encoded_sample['attention_mask'])\n","    # Get the predicted class (0 for negative, 1 for positive)\n","    _, predicted = torch.max(outputs, 1)\n","\n","# Extract the predicted class value\n","predicted_class = predicted.item()\n","\n","print(\"Input sentence:\", sample_sentence)\n","print(\"Predicted class:\", predicted_class)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744478348400,"user_tz":-120,"elapsed":14,"user":{"displayName":"k-j yli","userId":"05176293678065859194"}},"outputId":"73f1b0fe-d90c-4925-9039-9bde2ee106d5","id":"1FNGDg2WwyX1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input sentence: I completely detest this product, despite cheap price.\n","Predicted class: 0\n"]}]},{"cell_type":"markdown","source":["# Large dataset"],"metadata":{"id":"5MioMz_EOBpj"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from transformers import AutoTokenizer\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('punkt_tab')\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords\n","\n","# Preprocessing function (adjust as needed)\n","def preprocess_pandas(data, columns):\n","    df_ = pd.DataFrame(columns=columns)\n","    data['Sentence'] = data['Sentence'].str.lower()\n","    data['Sentence'] = data['Sentence'].replace('[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', regex=True)\n","    data['Sentence'] = data['Sentence'].replace('((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', regex=True)\n","    data['Sentence'] = data['Sentence'].str.replace('[^\\w\\s]', '', regex=True)\n","    data['Sentence'] = data['Sentence'].replace('\\d', '', regex=True)\n","    for index, row in data.iterrows():\n","        word_tokens = word_tokenize(row['Sentence'])\n","        filtered_sent = [w for w in word_tokens if not w in stopwords.words('english')]\n","        df_.loc[len(df_)] = {\n","            \"index\": row['index'],\n","            \"Class\": row['Class'],\n","            \"Sentence\": \" \".join(filtered_sent)\n","        }\n","    return df_\n","\n","# Load and preprocess your data\n","data = pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/amazon_cells_labelled_LARGE_25K.txt\",\n","                   delimiter='\\t', header=None)\n","data.columns = ['Sentence', 'Class']\n","data['index'] = data.index\n","columns = ['index', 'Class', 'Sentence']\n","data = preprocess_pandas(data, columns)\n","\n","# First split: 60% training and 40% temporary data.\n","train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n","    data['Sentence'].values.astype('U'),\n","    data['Class'].values.astype('int32'),\n","    test_size=0.40,\n","    random_state=0,\n","    shuffle=True\n",")\n","\n","# Second split: split temporary data equally into 50% validation and 50% test (i.e. 20% each overall)\n","val_texts, test_texts, val_labels, test_labels = train_test_split(\n","    temp_texts,\n","    temp_labels,\n","    test_size=0.5,\n","    random_state=0,\n","    shuffle=True\n",")\n","\n","# Initialize the transformer tokenizer (e.g., BERT)\n","tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Tokenize texts for each set (with padding and truncation)\n","encoded_train = tokenizer(list(train_texts), padding=True, truncation=True, return_tensors='pt')\n","encoded_val = tokenizer(list(val_texts), padding=True, truncation=True, return_tensors='pt')\n","encoded_test = tokenizer(list(test_texts), padding=True, truncation=True, return_tensors='pt')\n","\n","# Convert labels to tensors\n","train_y_tensor = torch.tensor(train_labels).long()\n","val_y_tensor = torch.tensor(val_labels).long()\n","test_y_tensor = torch.tensor(test_labels).long()\n","\n","# Create TensorDatasets for training, validation, and test sets\n","train_dataset = TensorDataset(encoded_train['input_ids'],\n","                              encoded_train['attention_mask'],\n","                              train_y_tensor)\n","val_dataset = TensorDataset(encoded_val['input_ids'],\n","                            encoded_val['attention_mask'],\n","                            val_y_tensor)\n","test_dataset = TensorDataset(encoded_test['input_ids'],\n","                             encoded_test['attention_mask'],\n","                             test_y_tensor)\n","\n","# Create DataLoaders\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"],"metadata":{"id":"nvLjmdWAus9K","executionInfo":{"status":"ok","timestamp":1744478438580,"user_tz":-120,"elapsed":84885,"user":{"displayName":"k-j yli","userId":"05176293678065859194"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"bb4be688-5f06-4151-a72f-42d5acd8a85f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n","\n","class TransformerClassifier(nn.Module):\n","    def __init__(self, transformer_model_name, hidden_dim, output_dim, dropout_rate=0.3):\n","        super(TransformerClassifier, self).__init__()\n","        self.transformer = AutoModel.from_pretrained(transformer_model_name)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.fc1 = nn.Linear(self.transformer.config.hidden_size, hidden_dim)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n","        cls_embedding = outputs.last_hidden_state[:, 0, :]\n","        out = self.dropout(cls_embedding)\n","        out = self.fc1(out)\n","        out = self.relu(out)\n","        out = self.dropout(out)\n","        out = self.fc2(out)\n","        return out\n","\n","# Set model parameters\n","hidden_dim = 20          # Experiment with other sizes as needed\n","output_dim = 2           # Binary classification: 0 (negative), 1 (positive)\n","model = TransformerClassifier('bert-base-uncased', hidden_dim, output_dim, dropout_rate=0.3)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=2e-5, weight_decay=1e-2)\n","\n","num_epochs = 10\n","total_steps = len(train_loader) * num_epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                            num_warmup_steps=int(0.1 * total_steps),\n","                                            num_training_steps=total_steps)\n","\n","# ---- Early Stopping Configuration ----\n","early_stopping_patience = 3  # Number of epochs with no improvement after which training will be stopped\n","best_val_loss = float('inf')\n","epochs_no_improve = 0\n","freeze_epochs = 2  # Freeze transformer layers for first few epochs\n","\n","for epoch in range(num_epochs):\n","\n","      # Freeze the transformer layers for the first few epochs\n","    if epoch < freeze_epochs:\n","        for param in model.transformer.parameters():\n","            param.requires_grad = False\n","    else:\n","        for param in model.transformer.parameters():\n","            param.requires_grad = True\n","    model.train()\n","    train_loss = 0.0\n","    for batch in train_loader:\n","        # Unpack the batch and move tensors to GPU\n","        input_ids, attention_mask, labels = batch\n","        input_ids = input_ids.to(device)\n","        attention_mask = attention_mask.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()       # Reset gradients\n","        outputs = model(input_ids, attention_mask)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()  # Update learning rate\n","        train_loss += loss.item()\n","\n","    avg_train_loss = train_loss / len(train_loader)\n","\n","    # ---- Validation Phase ----\n","    model.eval()\n","    val_loss = 0.0\n","    correct_val = 0\n","    total_val = 0\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            input_ids, attention_mask, labels = batch\n","            input_ids = input_ids.to(device)\n","            attention_mask = attention_mask.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(input_ids, attention_mask)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","            _, predicted = torch.max(outputs, 1)\n","            total_val += labels.size(0)\n","            correct_val += (predicted == labels).sum().item()\n","\n","    avg_val_loss = val_loss / len(val_loader)\n","    val_accuracy = 100 * correct_val / total_val\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs}] -> Train Loss: {avg_train_loss:.4f} | '\n","          f'Val Loss: {avg_val_loss:.4f} | Val Accuracy: {val_accuracy:.2f}%')\n","\n","    # ---- Early Stopping Check ----\n","    if avg_val_loss < best_val_loss:\n","        best_val_loss = avg_val_loss\n","        epochs_no_improve = 0\n","        # Save the current best model checkpoint\n","        torch.save(model.state_dict(), 'best_model_transformer.pt')\n","        print(\"Validation loss improved, saving model checkpoint.\")\n","    else:\n","        epochs_no_improve += 1\n","        print(f\"No improvement in validation loss for {epochs_no_improve} consecutive epoch(s).\")\n","        if epochs_no_improve >= early_stopping_patience:\n","            print(\"Early stopping triggered. Stopping training.\")\n","            break\n","\n","# ---- Load Best Model & Test Evaluation ----\n","model.load_state_dict(torch.load('best_model_transformer.pt'))\n","model.eval()\n","correct_test = 0\n","total_test = 0\n","with torch.no_grad():\n","    for batch in test_loader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids = input_ids.to(device)\n","        attention_mask = attention_mask.to(device)\n","        labels = labels.to(device)\n","\n","        outputs = model(input_ids, attention_mask)\n","        _, predicted = torch.max(outputs, 1)\n","        total_test += labels.size(0)\n","        correct_test += (predicted == labels).sum().item()\n","\n","test_accuracy = 100 * correct_test / total_test\n","print(f'Test Accuracy: {test_accuracy:.2f}%')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kk9YumGfu-wV","executionInfo":{"status":"ok","timestamp":1744479664230,"user_tz":-120,"elapsed":1179841,"user":{"displayName":"k-j yli","userId":"05176293678065859194"}},"outputId":"ff61f481-e09f-459f-e811-204fa38f6d26"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/10] -> Train Loss: 0.6929 | Val Loss: 0.6670 | Val Accuracy: 61.54%\n","Validation loss improved, saving model checkpoint.\n","Epoch [2/10] -> Train Loss: 0.6684 | Val Loss: 0.6476 | Val Accuracy: 61.54%\n","Validation loss improved, saving model checkpoint.\n","Epoch [3/10] -> Train Loss: 0.4158 | Val Loss: 0.3170 | Val Accuracy: 86.82%\n","Validation loss improved, saving model checkpoint.\n","Epoch [4/10] -> Train Loss: 0.3150 | Val Loss: 0.3086 | Val Accuracy: 87.28%\n","Validation loss improved, saving model checkpoint.\n","Epoch [5/10] -> Train Loss: 0.2741 | Val Loss: 0.3035 | Val Accuracy: 87.62%\n","Validation loss improved, saving model checkpoint.\n","Epoch [6/10] -> Train Loss: 0.2481 | Val Loss: 0.3154 | Val Accuracy: 87.92%\n","No improvement in validation loss for 1 consecutive epoch(s).\n","Epoch [7/10] -> Train Loss: 0.2223 | Val Loss: 0.3322 | Val Accuracy: 87.40%\n","No improvement in validation loss for 2 consecutive epoch(s).\n","Epoch [8/10] -> Train Loss: 0.2003 | Val Loss: 0.3425 | Val Accuracy: 87.56%\n","No improvement in validation loss for 3 consecutive epoch(s).\n","Early stopping triggered. Stopping training.\n","Test Accuracy: 87.34%\n"]}]},{"cell_type":"code","source":["# Explicit test: examine a single review and its prediction\n","\n","# Define a sample review (or use one from your dataset)\n","sample_sentence = \"This product exceeded my expectations and works great!\"\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","# Tokenize the sample sentence (using the same tokenizer as during training)\n","encoded_sample = tokenizer(sample_sentence, padding=True, truncation=True, return_tensors='pt')\n","\n","# Move the tokenized inputs to the device (GPU if available)\n","encoded_sample = {key: val.to(device) for key, val in encoded_sample.items()}\n","\n","# Pass the tokenized input through the model to get predictions\n","with torch.no_grad():\n","    outputs = model(encoded_sample['input_ids'], encoded_sample['attention_mask'])\n","    # Get the predicted class (0 for negative, 1 for positive)\n","    _, predicted = torch.max(outputs, 1)\n","\n","# Extract the predicted class value\n","predicted_class = predicted.item()\n","\n","print(\"Input sentence:\", sample_sentence)\n","print(\"Predicted class:\", predicted_class)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744480048730,"user_tz":-120,"elapsed":69,"user":{"displayName":"k-j yli","userId":"05176293678065859194"}},"outputId":"ee961fe4-0206-4a34-dad7-6d85c9e20b45","id":"IzJB9wM94rs_"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input sentence: This product exceeded my expectations and works great!\n","Predicted class: 1\n"]}]},{"cell_type":"code","source":["# Explicit test: examine a single review and its prediction\n","\n","# Define a sample review (or use one from your dataset)\n","sample_sentence = \"I'm very disappointed with the product.\"\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","# Tokenize the sample sentence (using the same tokenizer as during training)\n","encoded_sample = tokenizer(sample_sentence, padding=True, truncation=True, return_tensors='pt')\n","\n","# Move the tokenized inputs to the device (GPU if available)\n","encoded_sample = {key: val.to(device) for key, val in encoded_sample.items()}\n","\n","# Pass the tokenized input through the model to get predictions\n","with torch.no_grad():\n","    outputs = model(encoded_sample['input_ids'], encoded_sample['attention_mask'])\n","    # Get the predicted class (0 for negative, 1 for positive)\n","    _, predicted = torch.max(outputs, 1)\n","\n","# Extract the predicted class value\n","predicted_class = predicted.item()\n","\n","print(\"Input sentence:\", sample_sentence)\n","print(\"Predicted class:\", predicted_class)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744480051978,"user_tz":-120,"elapsed":21,"user":{"displayName":"k-j yli","userId":"05176293678065859194"}},"outputId":"dc69d3ef-936a-4025-ca9d-d58c7a7ca41e","id":"oF7dsUCv-uRm"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input sentence: I'm very disappointed with the product.\n","Predicted class: 0\n"]}]},{"cell_type":"code","source":["# Define a sample review sentence\n","sample_sentence = \"I completely detest this product, despite its cheap price.\"\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","# Tokenize the sample sentence (using the same tokenizer as during training)\n","encoded_sample = tokenizer(sample_sentence, padding=True, truncation=True, return_tensors='pt')\n","\n","# Move the tokenized inputs to the device (GPU if available)\n","encoded_sample = {key: val.to(device) for key, val in encoded_sample.items()}\n","\n","# Pass the tokenized input through the model to get predictions\n","with torch.no_grad():\n","    outputs = model(encoded_sample['input_ids'], encoded_sample['attention_mask'])\n","    # Get the predicted class (0 for negative, 1 for positive)\n","    _, predicted = torch.max(outputs, 1)\n","\n","# Extract the predicted class value\n","predicted_class = predicted.item()\n","\n","print(\"Input sentence:\", sample_sentence)\n","print(\"Predicted class:\", predicted_class)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yKBlyHGU0mFJ","executionInfo":{"status":"ok","timestamp":1744480136125,"user_tz":-120,"elapsed":24,"user":{"displayName":"k-j yli","userId":"05176293678065859194"}},"outputId":"d2a70fb0-9353-40f3-cba6-5421ec4c1403"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input sentence: Very cheap product, and works okay.\n","Predicted class: 1\n"]}]},{"cell_type":"markdown","source":["# **Task 1.3 Comparison**"],"metadata":{"id":"moHv6jL1FlgC"}},{"cell_type":"markdown","source":["*Compare the performance of the two models and explain in which scenarios you would prefer one over the other.*\n","\n","The transformer implementation demonstrates superior performance in terms of both speed and accuracy. It converges in fewer epochs and generally achieves better accuracy on the test set. However, the difference in accuracy between the transformer and the TF-IDF+ANN model is relatively modest for this particular dataset. In contrast, the computational cost of the transformer is significantly higher. Therefore, if computational resources are limited, the TF-IDF+ANN model is preferable. On the other hand, if achieving higher accuracy is crucial and the sequential order of words substantially influences the neural networks performance, the transformer model is the better choice.\n","\n","*How did the two models complexity, accuracy, and efficiency differ? Did one model outperform the other in specific scenarios or tasks? If so, why?*\n","\n","The TF-IDF+ANN implementation is considerably less complex and more efficient. This efficiency is most noticeable during CPU-based training: 40 epochs using 20 neurons in the hidden layers can be completed in just a few seconds, whereas the transformer implementationalso using a configuration involving 20 neuronsrequires 20 minutes to complete only 6 epochs. Despite these differences in training time, both models achieve similar levels of accuracy.\n","\n","Moreover, one significant advantage of the transformer model is its ability to attain reasonable accuracy with less data. For instance, when training on a small dataset, the TF-IDF+ANN model reached an accuracy of 82% on the test set, while the transformer achieved 87%a level of performance comparable to that attained on a larger dataset (approximately 87.3% accuracy). Consequently, if the available training data is limited, the transformer model is likely to provide more reliable performance. Conversely, when ample training data is available but computational resources are constrained, the TF-IDF+ANN model can deliver satisfactory results with greater efficiency for simpler tasks.\n","\n","*What insights did you obtain concerning data amount to train? Embedding utilized? Architectural choices made?*\n","\n","Data amount:\n","TF-IDF+ANN: requires more data since it uses shallow, non-contextual features.\n","Transformers: Leverages pretrained contextual embeddings, often performing well with less training data.\n","\n","Embedding utilized:\n","TF-IDF: Simple, interpretable, however lacks context and nuance.\n","Transformers: Provides rich, context-aware embeddings that caputure linguistic features.\n","\n","Architectural choices:\n","TF-IDF+ANN: Lightweight feedforward network with TF-IDF inputs are fast and resource-efficient but less effective on complex tasks.\n","Transformers: Deep, pretrained model (BERT) with classification head are more computationally intensive yet delivers superior performance on nuanced tasks.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"aJ1kd2bcvukR"}}]}